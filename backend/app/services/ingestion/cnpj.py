"""
CNPJ Ingestion Service - IngestÃ£o de dados CNPJ para Bronze layer.

ServiÃ§o responsÃ¡vel por:
1. Baixar arquivos da Receita Federal
2. Processar CSVs e converter para Parquet
3. Upload para MinIO (Bronze layer)
4. Gerenciar versionamento
"""

from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd
from loguru import logger

from backend.app.core.cnpj_client import CNPJClient
from backend.app.core.storage_client import get_storage_client
from backend.app.domains.cnpj import CNPJEntityType


class CNPJIngestionService:
    """
    ServiÃ§o de ingestÃ£o de dados CNPJ.

    Workflow:
    1. Detectar Ãºltima versÃ£o disponÃ­vel
    2. Download arquivo(s) por entidade
    3. Descompactar e validar CSV
    4. Converter para Parquet
    5. Upload para MinIO Bronze
    6. Atualizar metadata
    7. Cleanup temporÃ¡rios
    """

    def __init__(self):
        """Inicializa o serviÃ§o de ingestÃ£o CNPJ."""
        self.client = CNPJClient()
        self.storage = get_storage_client()

    def get_latest_version(self) -> Optional[str]:
        """
        Detecta a Ãºltima versÃ£o disponÃ­vel na Receita Federal.

        Returns:
            str: VersÃ£o YYYY-MM (ex: "2025-10")
            None: Se falhar
        """
        return self.client.get_latest_version()

    def get_current_version(self) -> Optional[str]:
        """
        ObtÃ©m a versÃ£o atualmente no Bronze layer.

        Returns:
            str: VersÃ£o atual ou None se nÃ£o houver
        """
        try:
            # Ler metadata
            metadata_key = "cnpj/_metadata/version_control.json"
            metadata_json = self.storage.read_json_from_s3(
                bucket=self.storage.BUCKET_BRONZE, key=metadata_key
            )

            return metadata_json.get("current_version")

        except Exception as e:
            logger.warning(f"âš ï¸  NÃ£o foi possÃ­vel ler versÃ£o atual: {e}")
            return None

    def needs_update(self) -> tuple[bool, Optional[str], Optional[str]]:
        """
        Verifica se hÃ¡ nova versÃ£o disponÃ­vel.

        Returns:
            tuple: (needs_update: bool, current_version: str, latest_version: str)

        Example:
            >>> service = CNPJIngestionService()
            >>> needs_update, current, latest = service.needs_update()
            >>> if needs_update:
            ...     print(f"Atualizar de {current} para {latest}")
        """
        current = self.get_current_version()
        latest = self.get_latest_version()

        if latest is None:
            logger.error("âŒ NÃ£o foi possÃ­vel detectar Ãºltima versÃ£o")
            return False, current, None

        if current is None:
            logger.info(f"ðŸ“¥ Primeira ingestÃ£o: {latest}")
            return True, None, latest

        if current != latest:
            logger.info(f"ðŸ”„ Nova versÃ£o disponÃ­vel: {current} â†’ {latest}")
            return True, current, latest

        logger.info(f"âœ… VersÃ£o atual estÃ¡ atualizada: {current}")
        return False, current, latest

    def ingest_entity(
        self,
        version: str,
        entity: CNPJEntityType,
        file_index: Optional[int] = None,
    ) -> dict:
        """
        Ingere uma entidade (ou arquivo de uma entidade) para o Bronze.

        Args:
            version: VersÃ£o YYYY-MM
            entity: Tipo de entidade
            file_index: Ãndice do arquivo (0-9) se particionado

        Returns:
            dict: Metadados da ingestÃ£o

        Workflow:
            1. Download do ZIP
            2. ExtraÃ§Ã£o do CSV
            3. Leitura e validaÃ§Ã£o
            4. ConversÃ£o para Parquet
            5. Upload para MinIO
            6. Cleanup local
        """
        logger.info(
            f"ðŸš€ Iniciando ingestÃ£o: {entity.value} (versÃ£o: {version}, Ã­ndice: {file_index})"
        )

        try:
            # 1. Download e extraÃ§Ã£o
            csv_files = self.client.download_and_extract(
                version=version,
                entity=entity,
                file_index=file_index,
                keep_zip=False,  # Remove ZIP apÃ³s extrair
            )

            if not csv_files:
                logger.error(f"âŒ Nenhum arquivo CSV extraÃ­do para {entity.value}")
                return {"success": False, "error": "No CSV files extracted"}

            csv_path = csv_files[0]  # Pega o primeiro CSV
            logger.info(f"ðŸ“„ CSV extraÃ­do: {csv_path.name}")

            # 2. Ler CSV
            logger.info("ðŸ“– Lendo CSV...")
            df = self._read_csv(csv_path, entity)

            if df is None or df.empty:
                logger.error("âŒ Falha ao ler CSV ou DataFrame vazio")
                return {"success": False, "error": "Failed to read CSV"}

            logger.info(f"âœ… CSV lido: {len(df):,} linhas, {len(df.columns)} colunas")

            # 3. Upload para MinIO (diretamente do DataFrame)
            s3_key = self._get_s3_key(version, entity, file_index)
            logger.info(f"â˜ï¸  Fazendo upload para MinIO: {s3_key}")

            # Converter para Parquet em memÃ³ria
            import io

            parquet_buffer = io.BytesIO()
            df.to_parquet(
                parquet_buffer,
                engine="pyarrow",
                compression="snappy",
                index=False,
            )

            parquet_size_mb = len(parquet_buffer.getvalue()) / (1024 * 1024)
            logger.info(f"âœ… Parquet gerado: {parquet_size_mb:.1f} MB")

            # Upload para MinIO via MinIOClient wrapper
            parquet_buffer.seek(0)
            self.storage._client.upload_fileobj(
                file_obj=parquet_buffer,
                bucket_name=self.storage.BUCKET_BRONZE,
                object_name=s3_key,
            )

            logger.info(
                f"âœ… Upload completo: s3://{self.storage.BUCKET_BRONZE}/{s3_key}"
            )

            # 4. Cleanup local
            logger.info("ðŸ—‘ï¸  Limpando arquivos temporÃ¡rios...")
            csv_path.unlink(missing_ok=True)

            # 6. Retornar metadata
            metadata = {
                "success": True,
                "version": version,
                "entity": entity.value,
                "file_index": file_index,
                "s3_key": s3_key,
                "row_count": len(df),
                "columns": list(df.columns),
                "size_mb": parquet_size_mb,
                "timestamp": datetime.now().isoformat(),
            }

            logger.info(f"ðŸŽ‰ IngestÃ£o completa: {entity.value}")
            return metadata

        except Exception as e:
            logger.error(f"âŒ Erro na ingestÃ£o de {entity.value}: {e}")
            import traceback

            logger.debug(traceback.format_exc())
            return {"success": False, "error": str(e)}

    def _read_csv(
        self, csv_path: Path, entity: CNPJEntityType
    ) -> Optional[pd.DataFrame]:
        """
        LÃª arquivo CSV com configuraÃ§Ãµes especÃ­ficas para CNPJ.

        Os arquivos CNPJ da RF tÃªm caracterÃ­sticas especÃ­ficas:
        - Separador: ponto-e-vÃ­rgula (;)
        - Encoding: ISO-8859-1 (latin1)
        - Sem cabeÃ§alho (primeira linha Ã© dados)
        - Aspas: duplas (")

        Args:
            csv_path: Caminho do arquivo CSV
            entity: Tipo de entidade (para definir schema)

        Returns:
            pd.DataFrame: DataFrame com os dados
        """
        try:
            # ConfiguraÃ§Ãµes gerais para CSVs da RF
            df = pd.read_csv(
                csv_path,
                sep=";",
                encoding="ISO-8859-1",
                header=None,  # Sem cabeÃ§alho
                dtype=str,  # Ler tudo como string inicialmente
                na_values=["", "NULL"],
                keep_default_na=False,
                low_memory=False,
            )

            # Adicionar nomes de colunas baseado na entidade
            column_names = self._get_column_names(entity)
            if column_names and len(column_names) == len(df.columns):
                df.columns = column_names
            else:
                logger.warning(
                    f"âš ï¸  Schema esperado ({len(column_names)} colunas) "
                    f"nÃ£o bate com CSV ({len(df.columns)} colunas). "
                    f"Usando nomes genÃ©ricos."
                )
                df.columns = [f"col_{i}" for i in range(len(df.columns))]

            return df

        except Exception as e:
            logger.error(f"âŒ Erro ao ler CSV: {e}")
            return None

    def _get_column_names(self, entity: CNPJEntityType) -> list[str]:
        """
        Retorna nomes de colunas para cada entidade.

        Baseado na documentaÃ§Ã£o oficial:
        https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf

        Args:
            entity: Tipo de entidade

        Returns:
            list[str]: Lista de nomes de colunas
        """
        schemas = {
            CNPJEntityType.CNAES: ["codigo", "descricao"],
            CNPJEntityType.MUNICIPIOS: ["codigo", "descricao"],
            CNPJEntityType.NATUREZAS: ["codigo", "descricao"],
            CNPJEntityType.QUALIFICACOES: ["codigo", "descricao"],
            CNPJEntityType.PAISES: ["codigo", "descricao"],
            CNPJEntityType.MOTIVOS: ["codigo", "descricao"],
            # TODO: Adicionar schemas completos para entidades grandes
            # CNPJEntityType.EMPRESAS: [...],
            # CNPJEntityType.ESTABELECIMENTOS: [...],
            # CNPJEntityType.SOCIOS: [...],
        }

        return schemas.get(entity, [])

    def _get_s3_key(
        self, version: str, entity: CNPJEntityType, file_index: Optional[int]
    ) -> str:
        """
        ConstrÃ³i chave S3 para armazenamento no Bronze.

        Estrutura:
            cnpj/{entity}/version={version}/
                data.parquet                    # Se arquivo Ãºnico
                part_{index}.parquet            # Se particionado

        Args:
            version: YYYY-MM
            entity: Tipo de entidade
            file_index: Ãndice do arquivo (se particionado)

        Returns:
            str: Chave S3 completa

        Examples:
            >>> _get_s3_key("2025-10", CNPJEntityType.CNAES, None)
            'cnpj/cnaes/version=2025-10/data.parquet'

            >>> _get_s3_key("2025-10", CNPJEntityType.EMPRESAS, 0)
            'cnpj/empresas/version=2025-10/part_0.parquet'
        """
        base_path = f"cnpj/{entity.value}/version={version}"

        if file_index is not None:
            filename = f"part_{file_index}.parquet"
        else:
            filename = "data.parquet"

        return f"{base_path}/{filename}"

    def ingest_full_entity(self, version: str, entity: CNPJEntityType) -> dict:
        """
        Ingere todos os arquivos de uma entidade.

        Para entidades particionadas (Empresas, Estabelecimentos, SÃ³cios),
        processa todos os arquivos (0-9).

        Para entidades Ãºnicas, processa o arquivo Ãºnico.

        Args:
            version: VersÃ£o YYYY-MM
            entity: Tipo de entidade

        Returns:
            dict: Resultado agregado da ingestÃ£o
        """
        logger.info(f"ðŸš€ IngestÃ£o completa: {entity.descricao} (versÃ£o {version})")

        num_files = entity.get_num_files()
        results = []

        if num_files == 1:
            # Arquivo Ãºnico
            result = self.ingest_entity(version, entity, file_index=None)
            results.append(result)
        else:
            # MÃºltiplos arquivos (0-9)
            for i in range(num_files):
                logger.info(f"ðŸ“¦ Processando arquivo {i+1}/{num_files}...")
                result = self.ingest_entity(version, entity, file_index=i)
                results.append(result)

        # Agregar resultados
        successes = sum(1 for r in results if r.get("success"))
        failures = len(results) - successes
        total_rows = sum(r.get("row_count", 0) for r in results if r.get("success"))

        summary = {
            "entity": entity.value,
            "version": version,
            "total_files": num_files,
            "successes": successes,
            "failures": failures,
            "total_rows": total_rows,
            "results": results,
        }

        logger.info(
            f"âœ… IngestÃ£o completa: {entity.value} "
            f"({successes}/{num_files} sucesso, {total_rows:,} linhas)"
        )

        return summary
